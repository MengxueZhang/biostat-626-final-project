---
title: "626_project"
author: "Chunhuigu"
date: "2024-04-07"
output: html_document
---


**Multicolinearity and Normalization- 1**. **partial data, threshold =5**

```{r}
library(ISLR)
library(MASS) # For lda function
library(caret)
library(e1071)
library(caret)
library(readxl)

pd <- read_excel("/Users/chunhuigu/Desktop/train/Aggregated_data_test.xlsx")
str(pd[, -1, drop = FALSE])
pd_x <- as.data.frame(pd[, -1])
vif_results <- usdm::vifstep(pd_x, th=5)
vif_results
vif_final <- vif_results@results
final_variable_names <- vif_final$Variables
final_data <- pd_x[, final_variable_names, drop = FALSE]

pre_proc_val <- preProcess(final_data, method = c("center", "scale"))
normalized_x <- predict(pre_proc_val, final_data)
pd1 <- cbind(pd[,1], normalized_x)
```

**Model Selection - 1** 

```{r}
library(caret)
library(nnet) # 神经网络模型

# 添加神经网络到模型方法列表
model_methods <- c(
  least_squares = "lm",
  bayes = "bayesglm",
  lasso = "glmnet",
  ridge = "glmnet",
  knn = "knn",
  nnet = "nnet" # 添加神经网络模型
)

# 初始化一个空列表来存储结果
results <- list()

# 设置交叉验证的训练控制
train_control <- trainControl(method = "cv", number = 10, savePredictions = "final")

# 执行10折交叉验证并计算测试集的RMSE和MAE
perform_cv <- function(model_method, data, outcome_var, ...) {
  set.seed(123) # 确保结果可重复
  
  # 训练模型，使用10折交叉验证
  model <- train(reformulate(". - survival_time", response = outcome_var), 
                 data = data, 
                 method = model_method, 
                 trControl = train_control,
                 ...)
  
  # 提取测试集的预测
  test_predictions <- model$pred[model$pred$Resample != "Resample01",]
  
  # 计算测试集的RMSE和MAE
  rmse_test <- RMSE(test_predictions$pred, test_predictions$obs)
  mae_test <- MAE(test_predictions$pred, test_predictions$obs)
  
  # 返回测试集的RMSE和MAE
  return(c(RMSE = rmse_test, MAE = mae_test))
}

# 对每个回归模型应用函数并存储测试RMSE和MAE
for (method_name in names(model_methods)) {
  if (method_name %in% c("lasso", "ridge")) {
    # 对lasso和ridge回归，指定alpha值
    alpha_value <- ifelse(method_name == "lasso", 1, 0)
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd1, "survival_time", 
                                         tuneLength = 3,
                                         tuneGrid = expand.grid(alpha = alpha_value, lambda = seq(0.001, 0.1, length = 10)))
  } else if (method_name == "nnet") {
    # 对神经网络，设置一个合适的隐藏层节点数和最大迭代次数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd1, "survival_time",
                                         trace = FALSE,
                                         maxit = 1000,
                                         linout = TRUE, # 线性输出对于回归问题
                    
                                         tuneLength = 3)
  } else {
    # 对其他方法，不需要指定额外参数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd1, "survival_time")
  }
}

# Add Poisson regression manually using a custom model specification in caret
set.seed(123)
poisson_model <- train(survival_time ~ ., data = pd1, method = "glm", family = "poisson", trControl = train_control)
poisson_test_predictions <- poisson_model$pred[poisson_model$pred$Resample != "Resample01",]
results[["poisson"]] <- c(RMSE = RMSE(poisson_test_predictions$pred, poisson_test_predictions$obs),
                          MAE = MAE(poisson_test_predictions$pred, poisson_test_predictions$obs))

# Compile test RMSE and MAE results into a data frame for comparison
results_df <- data.frame(Model = names(results), t(sapply(results, unlist)))
rownames(results_df) <- NULL # Reset row names to default

# Output results
print(results_df)
```
**train deep neural network -1**

```{r}
library(keras)

# 确定特征数量
num_features <- ncol(pd1) - 1

# 定义模型
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(num_features)) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 1)

# 编译模型，使用标准的RMSprop优化器
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_rmsprop(),  # 使用标准的RMSprop优化器
  metrics = c('mean_absolute_error')
)

# 10折交叉验证准备
k <- 10  # 折数
fold_size <- nrow(pd1) / k
cvscores_rmse <- vector()
cvscores_mae <- vector()

for (i in 1:k) {
  # 分割验证集和训练集
  val_indices <- ((i - 1) * fold_size + 1):(i * fold_size)
  val_data <- pd1[val_indices, ]
  train_data <- pd1[-val_indices, ]
  
  # 训练模型
  history <- model %>% fit(
    x = as.matrix(train_data[, -1]),  # 特征数据，移除第一列（目标变量）
    y = train_data[, 1],              # 目标变量
    epochs = 100,
    batch_size = 10,
    verbose = 0
  )
  
  # 评估模型
  scores <- model %>% evaluate(
    x = as.matrix(val_data[, -1]),
    y = val_data[, 1],
    verbose = 0
  )

  # 判断scores是否为list并且包含'loss'和'mean_absolute_error'
  if (is.list(scores) && "loss" %in% names(scores)) {
    rmse <- sqrt(scores$loss)
    mae <- scores$mean_absolute_error
  } else {  # 如果scores不是list，直接使用索引
    rmse <- sqrt(scores[1])
    mae <- scores[2]
  }
  
  # 将结果添加到vectors中
  cvscores_rmse <- c(cvscores_rmse, rmse)
  cvscores_mae <- c(cvscores_mae, mae)
}

# 计算平均RMSE和MAE
mean_rmse <- mean(cvscores_rmse)
mean_mae <- mean(cvscores_mae)

# 输出结果
cat("平均RMSE:", mean_rmse, "\n")
cat("平均MAE:", mean_mae, "\n")

```

**Multicolinearity - 2** **partial data, th = 10**

```{r}
library(ISLR)
library(MASS) # For lda function
library(caret)
library(e1071)
library(caret)
library(readxl)

pd <- read_excel("/Users/chunhuigu/Desktop/train/Aggregated_data_test.xlsx")
str(pd[, -1, drop = FALSE])
pd_x <- as.data.frame(pd[, -1])
vif_results2 <- usdm::vifstep(pd_x, th=10)
vif_results2
vif_final2 <- vif_results2@results
final_variable_names2 <- vif_final2$Variables
final_data2 <- pd_x[, final_variable_names2, drop = FALSE]
pre_proc_val2 <- preProcess(final_data2, method = c("center", "scale"))
normalized_x2 <- predict(pre_proc_val2, final_data2)
pd2 <- cbind(pd[,1], normalized_x2)
```

**Model Selection - 2**

```{r}
library(caret)
library(nnet) # 神经网络模型

# 添加神经网络到模型方法列表
model_methods <- c(
  least_squares = "lm",
  bayes = "bayesglm",
  lasso = "glmnet",
  ridge = "glmnet",
  knn = "knn",
  nnet = "nnet" # 添加神经网络模型
)

# 初始化一个空列表来存储结果
results <- list()

# 设置交叉验证的训练控制
train_control <- trainControl(method = "cv", number = 10, savePredictions = "final")

# 执行10折交叉验证并计算测试集的RMSE和MAE
perform_cv <- function(model_method, data, outcome_var, ...) {
  set.seed(123) # 确保结果可重复
  
  # 训练模型，使用10折交叉验证
  model <- train(reformulate(". - survival_time", response = outcome_var), 
                 data = data, 
                 method = model_method, 
                 trControl = train_control,
                 ...)
  
  # 提取测试集的预测
  test_predictions <- model$pred[model$pred$Resample != "Resample01",]
  
  # 计算测试集的RMSE和MAE
  rmse_test <- RMSE(test_predictions$pred, test_predictions$obs)
  mae_test <- MAE(test_predictions$pred, test_predictions$obs)
  
  # 返回测试集的RMSE和MAE
  return(c(RMSE = rmse_test, MAE = mae_test))
}

# 对每个回归模型应用函数并存储测试RMSE和MAE
for (method_name in names(model_methods)) {
  if (method_name %in% c("lasso", "ridge")) {
    # 对lasso和ridge回归，指定alpha值
    alpha_value <- ifelse(method_name == "lasso", 1, 0)
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd2, "survival_time", 
                                         tuneLength = 3,
                                         tuneGrid = expand.grid(alpha = alpha_value, lambda = seq(0.001, 0.1, length = 10)))
  } else if (method_name == "nnet") {
    # 对神经网络，设置一个合适的隐藏层节点数和最大迭代次数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd2, "survival_time",
                                         trace = FALSE,
                                         maxit = 1000,
                                         linout = TRUE, # 线性输出对于回归问题
                    
                                         tuneLength = 3)
  } else {
    # 对其他方法，不需要指定额外参数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd2, "survival_time")
  }
}

# Add Poisson regression manually using a custom model specification in caret
set.seed(123)
poisson_model <- train(survival_time ~ ., data = pd2, method = "glm", family = "poisson", trControl = train_control)
poisson_test_predictions <- poisson_model$pred[poisson_model$pred$Resample != "Resample01",]
results[["poisson"]] <- c(RMSE = RMSE(poisson_test_predictions$pred, poisson_test_predictions$obs),
                          MAE = MAE(poisson_test_predictions$pred, poisson_test_predictions$obs))

# Compile test RMSE and MAE results into a data frame for comparison
results_df <- data.frame(Model = names(results), t(sapply(results, unlist)))
rownames(results_df) <- NULL # Reset row names to default

# Output results
print(results_df)
```

**train deep neural network - 2**

```{r}
library(keras)

# 确定特征数量
num_features <- ncol(pd2) - 1

# 定义模型
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(num_features)) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 1)

# 编译模型，使用标准的RMSprop优化器
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_rmsprop(),  # 使用标准的RMSprop优化器
  metrics = c('mean_absolute_error')
)

# 10折交叉验证准备
k <- 10  # 折数
fold_size <- nrow(pd2) / k
cvscores_rmse <- vector()
cvscores_mae <- vector()

for (i in 1:k) {
  # 分割验证集和训练集
  val_indices <- ((i - 1) * fold_size + 1):(i * fold_size)
  val_data <- pd2[val_indices, ]
  train_data <- pd2[-val_indices, ]
  
  # 训练模型
  history <- model %>% fit(
    x = as.matrix(train_data[, -1]),  # 特征数据，移除第一列（目标变量）
    y = train_data[, 1],              # 目标变量
    epochs = 100,
    batch_size = 10,
    verbose = 0
  )
  
  # 评估模型
  scores <- model %>% evaluate(
    x = as.matrix(val_data[, -1]),
    y = val_data[, 1],
    verbose = 0
  )

  # 判断scores是否为list并且包含'loss'和'mean_absolute_error'
  if (is.list(scores) && "loss" %in% names(scores)) {
    rmse <- sqrt(scores$loss)
    mae <- scores$mean_absolute_error
  } else {  # 如果scores不是list，直接使用索引
    rmse <- sqrt(scores[1])
    mae <- scores[2]
  }
  
  # 将结果添加到vectors中
  cvscores_rmse <- c(cvscores_rmse, rmse)
  cvscores_mae <- c(cvscores_mae, mae)
}

# 计算平均RMSE和MAE
mean_rmse <- mean(cvscores_rmse)
mean_mae <- mean(cvscores_mae)

# 输出结果
cat("平均RMSE:", mean_rmse, "\n")
cat("平均MAE:", mean_mae, "\n")

```


**Multicolinearity - 3** **whole data, th = 5**

```{r}
library(ISLR)
library(MASS) # For lda function
library(caret)
library(e1071)
library(caret)
library(readxl)

pd <- read_excel("/Users/chunhuigu/Desktop/train/Whole_data.xlsx")
str(pd[, -1, drop = FALSE])
pd_x <- as.data.frame(pd[, -1])
vif_results3 <- usdm::vifstep(pd_x, th=5)
vif_results3
vif_final3 <- vif_results3@results
final_variable_names3 <- vif_final3$Variables
final_data3 <- pd_x[, final_variable_names3, drop = FALSE]
pre_proc_val3 <- preProcess(final_data3, method = c("center", "scale"))
normalized_x3 <- predict(pre_proc_val3, final_data3)
pd3 <- cbind(pd[,1], normalized_x3)
```

**Model Selection - 3** 

```{r}
library(caret)
library(nnet) # 神经网络模型

# 添加神经网络到模型方法列表
model_methods <- c(
  least_squares = "lm",
  bayes = "bayesglm",
  lasso = "glmnet",
  ridge = "glmnet",
  knn = "knn",
  nnet = "nnet" # 添加神经网络模型
)

# 初始化一个空列表来存储结果
results <- list()

# 设置交叉验证的训练控制
train_control <- trainControl(method = "cv", number = 10, savePredictions = "final")

# 执行10折交叉验证并计算测试集的RMSE和MAE
perform_cv <- function(model_method, data, outcome_var, ...) {
  set.seed(123) # 确保结果可重复
  
  # 训练模型，使用10折交叉验证
  model <- train(reformulate(". - survival_time", response = outcome_var), 
                 data = data, 
                 method = model_method, 
                 trControl = train_control,
                 ...)
  
  # 提取测试集的预测
  test_predictions <- model$pred[model$pred$Resample != "Resample01",]
  
  # 计算测试集的RMSE和MAE
  rmse_test <- RMSE(test_predictions$pred, test_predictions$obs)
  mae_test <- MAE(test_predictions$pred, test_predictions$obs)
  
  # 返回测试集的RMSE和MAE
  return(c(RMSE = rmse_test, MAE = mae_test))
}

# 对每个回归模型应用函数并存储测试RMSE和MAE
for (method_name in names(model_methods)) {
  if (method_name %in% c("lasso", "ridge")) {
    # 对lasso和ridge回归，指定alpha值
    alpha_value <- ifelse(method_name == "lasso", 1, 0)
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd3, "survival_time", 
                                         tuneLength = 3,
                                         tuneGrid = expand.grid(alpha = alpha_value, lambda = seq(0.001, 0.1, length = 10)))
  } else if (method_name == "nnet") {
    # 对神经网络，设置一个合适的隐藏层节点数和最大迭代次数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd3, "survival_time",
                                         trace = FALSE,
                                         maxit = 1000,
                                         linout = TRUE, # 线性输出对于回归问题
                    
                                         tuneLength = 3)
  } else {
    # 对其他方法，不需要指定额外参数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd3, "survival_time")
  }
}

# Add Poisson regression manually using a custom model specification in caret
set.seed(123)
poisson_model <- train(survival_time ~ ., data = pd3, method = "glm", family = "poisson", trControl = train_control)
poisson_test_predictions <- poisson_model$pred[poisson_model$pred$Resample != "Resample01",]
results[["poisson"]] <- c(RMSE = RMSE(poisson_test_predictions$pred, poisson_test_predictions$obs),
                          MAE = MAE(poisson_test_predictions$pred, poisson_test_predictions$obs))

# Compile test RMSE and MAE results into a data frame for comparison
results_df <- data.frame(Model = names(results), t(sapply(results, unlist)))
rownames(results_df) <- NULL # Reset row names to default

# Output results
print(results_df)

```

**train deep neural network - 3**

```{r}
library(keras)

# 确定特征数量
num_features <- ncol(pd3) - 1

# 定义模型
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(num_features)) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 1)

# 编译模型，使用标准的RMSprop优化器
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_rmsprop(),  # 使用标准的RMSprop优化器
  metrics = c('mean_absolute_error')
)

# 10折交叉验证准备
k <- 10  # 折数
fold_size <- nrow(pd3) / k
cvscores_rmse <- vector()
cvscores_mae <- vector()

for (i in 1:k) {
  # 分割验证集和训练集
  val_indices <- ((i - 1) * fold_size + 1):(i * fold_size)
  val_data <- pd3[val_indices, ]
  train_data <- pd3[-val_indices, ]
  
  # 训练模型
  history <- model %>% fit(
    x = as.matrix(train_data[, -1]),  # 特征数据，移除第一列（目标变量）
    y = train_data[, 1],              # 目标变量
    epochs = 100,
    batch_size = 10,
    verbose = 0
  )
  
  # 评估模型
  scores <- model %>% evaluate(
    x = as.matrix(val_data[, -1]),
    y = val_data[, 1],
    verbose = 0
  )

  # 判断scores是否为list并且包含'loss'和'mean_absolute_error'
  if (is.list(scores) && "loss" %in% names(scores)) {
    rmse <- sqrt(scores$loss)
    mae <- scores$mean_absolute_error
  } else {  # 如果scores不是list，直接使用索引
    rmse <- sqrt(scores[1])
    mae <- scores[2]
  }
  
  # 将结果添加到vectors中
  cvscores_rmse <- c(cvscores_rmse, rmse)
  cvscores_mae <- c(cvscores_mae, mae)
}

# 计算平均RMSE和MAE
mean_rmse <- mean(cvscores_rmse)
mean_mae <- mean(cvscores_mae)

# 输出结果
cat("平均RMSE:", mean_rmse, "\n")
cat("平均MAE:", mean_mae, "\n")

```

**Multicolinearity - 4** **whole data, th = 10**

```{r}
library(ISLR)
library(MASS) # For lda function
library(caret)
library(e1071)
library(caret)
library(readxl)

pd <- read_excel("/Users/chunhuigu/Desktop/train/Whole_data.xlsx")
str(pd[, -1, drop = FALSE])
pd_x <- as.data.frame(pd[, -1])
vif_results4 <- usdm::vifstep(pd_x, th=10)
vif_results4
vif_final4 <- vif_results4@results
final_variable_names4 <- vif_final4$Variables
final_data4 <- pd_x[, final_variable_names4, drop = FALSE]
pd4 <- cbind(pd[,1], final_data4)
```

**Model Selection - 4** 

```{r}
library(caret)
library(nnet) # 神经网络模型

# 添加神经网络到模型方法列表
model_methods <- c(
  least_squares = "lm",
  bayes = "bayesglm",
  lasso = "glmnet",
  ridge = "glmnet",
  knn = "knn",
  nnet = "nnet" # 添加神经网络模型
)

# 初始化一个空列表来存储结果
results <- list()

# 设置交叉验证的训练控制
train_control <- trainControl(method = "cv", number = 10, savePredictions = "final")

# 执行10折交叉验证并计算测试集的RMSE和MAE
perform_cv <- function(model_method, data, outcome_var, ...) {
  set.seed(123) # 确保结果可重复
  
  # 训练模型，使用10折交叉验证
  model <- train(reformulate(". - survival_time", response = outcome_var), 
                 data = data, 
                 method = model_method, 
                 trControl = train_control,
                 ...)
  
  # 提取测试集的预测
  test_predictions <- model$pred[model$pred$Resample != "Resample01",]
  
  # 计算测试集的RMSE和MAE
  rmse_test <- RMSE(test_predictions$pred, test_predictions$obs)
  mae_test <- MAE(test_predictions$pred, test_predictions$obs)
  
  # 返回测试集的RMSE和MAE
  return(c(RMSE = rmse_test, MAE = mae_test))
}

# 对每个回归模型应用函数并存储测试RMSE和MAE
for (method_name in names(model_methods)) {
  if (method_name %in% c("lasso", "ridge")) {
    # 对lasso和ridge回归，指定alpha值
    alpha_value <- ifelse(method_name == "lasso", 1, 0)
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd4, "survival_time", 
                                         tuneLength = 3,
                                         tuneGrid = expand.grid(alpha = alpha_value, lambda = seq(0.001, 0.1, length = 10)))
  } else if (method_name == "nnet") {
    # 对神经网络，设置一个合适的隐藏层节点数和最大迭代次数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd4, "survival_time",
                                         trace = FALSE,
                                         maxit = 1000,
                                         linout = TRUE, # 线性输出对于回归问题
                    
                                         tuneLength = 3)
  } else {
    # 对其他方法，不需要指定额外参数
    results[[method_name]] <- perform_cv(model_methods[[method_name]], pd4, "survival_time")
  }
}

# Add Poisson regression manually using a custom model specification in caret
set.seed(123)
poisson_model <- train(survival_time ~ ., data = pd4, method = "glm", family = "poisson", trControl = train_control)
poisson_test_predictions <- poisson_model$pred[poisson_model$pred$Resample != "Resample01",]
results[["poisson"]] <- c(RMSE = RMSE(poisson_test_predictions$pred, poisson_test_predictions$obs),
                          MAE = MAE(poisson_test_predictions$pred, poisson_test_predictions$obs))

# Compile test RMSE and MAE results into a data frame for comparison
results_df <- data.frame(Model = names(results), t(sapply(results, unlist)))
rownames(results_df) <- NULL # Reset row names to default

# Output results
print(results_df)

```

**train deep neural network - 4**

```{r}
library(keras)

# 确定特征数量
num_features <- ncol(pd4) - 1

# 定义模型
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(num_features)) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 1)

# 编译模型，使用标准的RMSprop优化器
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_rmsprop(),  # 使用标准的RMSprop优化器
  metrics = c('mean_absolute_error')
)

# 10折交叉验证准备
k <- 10  # 折数
fold_size <- nrow(pd4) / k
cvscores_rmse <- vector()
cvscores_mae <- vector()

for (i in 1:k) {
  # 分割验证集和训练集
  val_indices <- ((i - 1) * fold_size + 1):(i * fold_size)
  val_data <- pd4[val_indices, ]
  train_data <- pd4[-val_indices, ]
  
  # 训练模型
  history <- model %>% fit(
    x = as.matrix(train_data[, -1]),  # 特征数据，移除第一列（目标变量）
    y = train_data[, 1],              # 目标变量
    epochs = 100,
    batch_size = 10,
    verbose = 0
  )
  
  # 评估模型
  scores <- model %>% evaluate(
    x = as.matrix(val_data[, -1]),
    y = val_data[, 1],
    verbose = 0
  )

  # 判断scores是否为list并且包含'loss'和'mean_absolute_error'
  if (is.list(scores) && "loss" %in% names(scores)) {
    rmse <- sqrt(scores$loss)
    mae <- scores$mean_absolute_error
  } else {  # 如果scores不是list，直接使用索引
    rmse <- sqrt(scores[1])
    mae <- scores[2]
  }
  
  # 将结果添加到vectors中
  cvscores_rmse <- c(cvscores_rmse, rmse)
  cvscores_mae <- c(cvscores_mae, mae)
}

# 计算平均RMSE和MAE
mean_rmse <- mean(cvscores_rmse)
mean_mae <- mean(cvscores_mae)

# 输出结果
cat("平均RMSE:", mean_rmse, "\n")
cat("平均MAE:", mean_mae, "\n")

```